---
sidebarDepth: 3
sidebar: auto
prev:
  text: Back To 目录
  link: /ElasticStack/
typora-root-url: ..\.vuepress\public
---



## es内置的分词器

::: tip

es内置有四种分词器

:::


```json
对下面这句话进行分词
Set the shape to semi-transparent by calling set_trans(5)

standard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5（默认的是standard）

simple analyzer：set, the, shape, to, semi, transparent, by, calling, set, trans

whitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)

stop analyzer: 移除停用词，比如a the it等等
```



测试

```json
POST _analyze
{
  "analyzer": "stop",
  "text": "Set the shape to semi-transparent by calling set_trans(5)"
}
```

----------

## 分词器工作流程

::: tip

1. 切分词语  
2. normalization(时态转换，单复数转换)

:::

给你一段句子，然后将这段句子拆分成一个一个的单个的单词，同时对每个单词进行normalization（时态转换，单复数转换）

```json
character filter：在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签（<span>hello<span> --> hello），& --> and （I&you --> I and you）

tokenizer：分词，hello you and me --> hello, you, and, me

token filter：lowercase，stop word，synonymom，liked --> like，Tom --> tom，a/the/an --> 干掉，small --> little
```

一个分词器，很重要，将一段文本进行各种处理，最后处理好的结果才会拿去建立倒排索引

## 定制分词器

### 默认的分词器

1. standard tokenizer：以单词边界进行切分
2. standard token filter：什么都不做
3. lowercase token filter：将所有字母转换为小写
4. stop token filer（默认被禁用）：移除停用词，比如a the it等等

### 修改分词器的设置

启用english停用词token filter

```json
# es_std 自己定义的一个名称
PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "es_std": {
          "type": "standard",
          "stopwords": "_english_"
        }
      }
    }
  }
}

GET /my_index/_analyze
{
  "analyzer": "standard", 
  "text": "a dog is in the house"
}

GET /my_index/_analyze
{
  "analyzer": "es_std",
  "text":"a dog is in the house"
}
```

​             

 定制化自己的分词器

```json
PUT /my_index
{
  "settings": {
    "analysis": {
      "char_filter": {
        "&_to_and": {
          "type": "mapping",
          "mappings": [
            "&=> and"
          ]
        }
      },
      "filter": {
        "my_stopwords": {
          "type": "stop",
          "stopwords": [
            "the",
            "a"
          ]
        }
      },
      "analyzer": {
        "my_analyzer": {
          "type": "custom",
          "char_filter": [
            "html_strip",
            "&_to_and"
          ],
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "my_stopwords"
          ]
        }
      }
    }
  }
}

GET /my_index/_analyze
{
  "text": "tom&jerry are a friend in the house, <a>, HAHA!!",
  "analyzer": "my_analyzer"
}
```

​              

## IK分词器

### ik配置文件

> IK分词器源码下载：https://github.com/medcl/elasticsearch-analysis-ik/tree         
>
> ik配置文件地址：es/plugins/ik/config目录    

1. main.dic：ik原生内置的中文词库，总共有27万多条，只要是这些单词，都会被分在一起
2. stopword.dic：英文停用词
3. suffix.dic：放了一些后缀
4. quantifier.dic：放了一些单位相关的词
5. surname.dic：中国的姓氏
6. IKAnalyzer.cfg.xml：用来配置自定义词库

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict"></entry>
	 <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords"></entry>
	<!--用户可以在这里配置远程扩展字典 -->
	<!-- <entry key="remote_ext_dict">words_location</entry> -->
	<!--用户可以在这里配置远程扩展停止词字典-->
	<!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>

```

ik原生最重要的两个配置文件

1. main.dic：包含了原生的中文词语，会按照这个里面的词语去分词
2. stopword.dic：包含了英文的停用词 (一般，像停用词，会在分词的时候，直接被干掉，不会建立在倒排索引中)



### IK分词器自定义词库❤️

#### 自己建立词库

自己建立词库：每年都会涌现一些特殊的流行词，网红，蓝瘦香菇，喊麦，鬼畜，一般不会在ik的原生词典里,需要自己补充自己的最新的词语，到ik的词库里面去

```xml
<entry key="ext_dict">custom/ext_stopword.dic</entry>
custom/ext_stopword.dic，已经有了常用的中文停用词，可以补充自己的停用词，然后重启es
```

补充自己的词语，然后需要重启es，才能生效

自己建立停用词库：比如了，的，啥，么，我们可能并不想去建立索引，让人家搜索

```xml
<entry key="ext_stopwords">custom/ext_stopword.dic</entry>
custom/ext_stopword.dic，已经有了常用的中文停用词，可以补充自己的停用词，然后重启es
```



### IK热更新

每次都是在es的扩展词典中，手动添加新词语，很坑

1. 每次添加完，都要重启es才能生效，非常麻烦
2. es是分布式的，可能有数百个节点，你不能每次都一个一个节点上面去修改

es不停机，直接我们在外部某个地方添加新的词语，es中立即热加载到这些新词语

