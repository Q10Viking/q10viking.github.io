---
sidebarDepth: 3
sidebar: auto
prev:
  text: Back To 目录
  link: /ElasticStack/
typora-root-url: ..\.vuepress\public
---

## 1. 集群搭建

本案例搭建三台集群（但由于实际操作发现内存不足只安装了两台），es的安装主要参考单机版本

配置集群，主要修改配置文件

```sh
cluster.name: my-application		#	集群名称保持一致
node.name: node1 			# 各自节点的名称	node.name: node2 node.name: node3   注意不要带上减号，如node-1
network.host: 0.0.0.0	# 允许所有IP访问
discovery.seed_hosts: ["IP1", "IP2", "IP3"] # 添加集群的IP discovery.seed_hosts: ["192.168.88.173","192.168.88.174","192.168.88.175"]
cluster.initial_master_nodes: ["节点1名称", "节点2名称", "节点3名称"] 
```



**注意： ** vmware克隆的虚拟机，需要将之前安装的elasticsearch的生成数据删除  

```sh
path.data: /usr/local/es/elasticsearch-7.6.1/data
目录下删除 nodes
```

## 2. 查看是否安装成功

### 2.1 linux控制台

```sh
curl -XGET "http://192.168.88.174:9200/_cluster/health?pretty" 
```

```json
{
  "cluster_name" : "my-application",
  "status" : "green",		//	为绿色
  "timed_out" : false,	
  "number_of_nodes" : 2,	//	节点
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 1,
  "active_shards" : 2,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
```

### 2.2 Kibana测试

```
GET   _cat/nodes?v
```

![image-20210502230249387](/images/elasticsearch/image-20210502230249387.png)

```
GET   _cat/health?v
```

![image-20210502230347481](/images/elasticsearch/image-20210502230347481.png)



## 3. Elasticsearch-head插件

1. elasticsearch-head这个插件是es提供的一个用于图形化界面查看的一个插件工具，可以安装上这个插件之后，通过这个插件来实现我们通过浏览器查看es当中的数据



在192.168.88.173这台机器上安装即可

### 3.1 安装node环境

```sh
# 下载node
wget https://npm.taobao.org/mirrors/node/v8.1.0/node-v8.1.0-linux-x64.tar.gz
# 解压
tar -zxvf node-v8.1.0-linux-x64.tar.gz
# 创建软连接
sudo ln -s /usr/local/es/node-v8.1.0-linux-x64/lib/node_modules/npm/bin/npm-cli.js /usr/local/bin/npm
sudo ln -s /usr/local/es/node-v8.1.0-linux-x64/bin/node /usr/local/bin/node

# 添加环境变量
vi /etc/profile
export NODE_HOME=/usr/local/es/node-v8.1.0-linux-x64
export PATH=:$PATH:$NODE_HOME/bin

# 生效文件
source /etc/profile

# 验证
node -v
npm -v
```



### 3.2 安装elasticsearch-head插件

```sh
# 解压
tar -zxvf elasticsearch-head-compile-after.tar.gz
# 修改Gruntfile.js这个文件
cd /usr/local/es/elasticsearch-head
vim Gruntfile.js

#============修改IP地址===================
hostname: '192.168.88.173'
#========================================

# 修改 app.js
cd /usr/local/es/elasticsearch-head/_site
vim app.js
在Vim中输入「:4354」，定位到第4354行，修改 http://localhost:9200为http://192.168.88.173:9200
```

### 3.1 启动

```sh
cd /usr/local/es/elasticsearch-head/node_modules/grunt/bin/

# 前台启动
./grunt server

#==============启动后的效果==================================
Running "connect:server" (connect) task
Waiting forever...
Started connect web server on http://192.168.88.173:9100

# 后台启动
nohup ./grunt server >/dev/null 2>&1 &
```

![image-20210502234442951](/images/elasticsearch/image-20210502234442951.png)



## 4 .集群状态

**如何快速了解集群的健康状况？green、yellow、red？**

green：每个索引的primary shard和replica shard都是active状态的

yellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态

red：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了



#### 4.1 黄色状态

集群什么情况会处于一个yellow状态？

1. 假设现在就一台linux服务器，就启动了一个es进程，相当于就只有一个node。
2. 现在es中有一个index，就是kibana自己内置建立的index。**由于默认的配置是给每个index分配1个primary shard和1个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）**。
3. 现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。

测试：启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态