---
sidebarDepth: 3
sidebar: auto
prev:
  text: Back To 目录
  link: /ElasticStack/
typora-root-url: ..\.vuepress\public
---



## **Elasticsearch的节点类型**

在Elasticsearch主要分成两类节点，一类是Master，一类是DataNode。

### **Master节点**

在Elasticsearch启动时，会选举出来一个Master节点。当某个节点启动后，然后使用Zen Discovery机制找到集群中的其他节点，并建立连接。 discovery.seed_hosts: ["192.168.21.130", "192.168.21.131", "192.168.21.132"]

并从候选主节点中选举出一个主节点。cluster.initial_master_nodes: ["node1", "node2","node3"]

Master节点主要负责：

1. **管理索引（创建索引、删除索引）、分配分片**
2. 维护元数据
3. 管理集群节点状态
4. **不负责数据写入和查询，比较轻量级**

一个Elasticsearch集群中，只有一个Master节点。在生产环境中，内存可以相对小一点，但机器要稳定



### **DataNode节点**

在Elasticsearch集群中，会有N个DataNode节点。DataNode节点主要负责：

1. 数据写入、数据检索，大部分Elasticsearch的压力都在DataNode节点上
2. 在生产环境中，内存最好配置大一些



### **分片（Shard）**

 **Elasticsearch是一个分布式的搜索引擎，索引的数据也是分成若干部分，分布在不同的服务器节点中**

一个索引（index）由多个shard（分片）组成，而分片是分布在不同的服务器上的,分布在不同服务器节点中的索引数据，就是分片（Shard）。

Elasticsearch会自动管理分片，如果发现分片分布不均衡，就会自动迁移



### **副本**

为了对Elasticsearch的分片进行容错，假设某个节点不可用，会导致整个索引库都将不可用。所以，需要对分片进行副本容错。每一个分片都会有对应的副本。

在Elasticsearch中，默认创建的索引为1个分片、每个分片有1个主分片和1个副本分片。

每个分片都会有一个Primary Shard（主分片），也会有若干个Replica Shard（副本分片）

Primary Shard和Replica Shard不在同一个节点上



### **指定分片、副本数量**

```json
PUT /job_idx_shard_temp
{
  "mappings": {
    "properties": {
      "id":{"type":"long","store":true},
      "area":{"type":"keyword","store":true},
      "exp":{"type":"keyword","store":true},
      "edu":{"type":"keyword","store":true},
      "salary":{"type":"keyword","store":true},
      "job_type":{"type":"keyword","store":true},
      "cmp":{"type":"keyword","store":true},
      "pv":{"type":"keyword","store":true},
      "title":{"type":"text","store":true},
      "jd":{"type":"text"}
    }
  },
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 2
  }
}
```

## 文档写入原理❤️

![image-20210503014715892](/images/elasticsearch/image-20210503014715892.png)

1. 选择任意一个DataNode发送请求，例如：node2。此时，node2就成为一个coordinating node（协调节点）
2. 计算得到文档要写入的分片: shard = hash(routing) % number_of_primary_shards   routing 是一个可变值，默认是文档的 _id
3. coordinating node会进行路由，将请求转发给对应的primary shard所在的DataNode（假设primary shard在node1、replica shard在node2）
4. node1节点上的Primary Shard处理请求，写入数据到索引库中，并将数据同步到Replica shard
5. Primary Shard和Replica Shard都保存好了文档，返回client

--------



## 检索原理❤️

![img](/images/elasticsearch/3511)

1.  client发起查询请求，某个DataNode接收到请求，该DataNode就会成为协调节点（Coordinating Node）
2.  协调节点（Coordinating Node）将查询请求广播到每一个数据节点，**这些数据节点的分片会处理该查询请求**
3.  每个分片进行数据查询，将符合条件的数据放在一个优先队列中，并将这些数据的文档ID、节点信息、分片信息返回给协调节点
4.  协调节点将所有的结果进行汇总，并进行全局排序
5.  协调节点向包含这些文档ID的分片发送get请求，对应的分片将文档数据返回给协调节点，最后协调节点将数据返回给客户端



## 准实时索引实现❤️

> 实时搜索：刚刚加入的数据，就可以被搜索出来

<img src="/images/elasticsearch/3554" alt="img" style="zoom: 50%;" />



### **溢写到文件系统缓存**

当数据写入到ES分片时，会首先写入到内存中，然后通过内存的buffer生成一个segment，并刷到**文件系统缓存**中，**数据可以被检索**（注意不是直接刷到磁盘,在内存Memory Buffer中还不能被检索）

ES中默认1秒，refresh一次

### **写translog保障容错**

在写入到内存中的同时，也会记录translog日志，在refresh期间出现异常，会根据translog来进行数据恢复

等到文件系统缓存中的segment数据都刷到磁盘中，清空translog文件

### **flush到磁盘**

ES默认每隔30分钟会将文件系统缓存的数据刷入到磁盘

### **segment合并**

 Segment太多时，ES定期会将多个segment合并成为大的segment，减少索引查询时IO开销，此阶段ES会真正的物理删除
